{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcLYHiGuWDRE",
    "outputId": "9c455bc0-5621-457f-e696-127ea6813d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/jan/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: datasets in /home/jan/.local/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/jan/.local/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/jan/.local/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: xxhash in /home/jan/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /home/jan/.local/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: filelock in /home/jan/.local/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jan/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: packaging in /home/jan/.local/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/jan/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jan/.local/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jan/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jan/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jan/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jan/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "qtNjmAy0VzMq",
    "outputId": "fb2b2f82-f1cd-495d-b61f-0048be982dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 10\n",
    "n_batches = 1200\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train')\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], max_length=max_len,truncation=True)\n",
    "\n",
    "dataset = dataset.filter(lambda example: example['text'] is not None and example['text'].strip() != '')\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ").select(range(batch_size*n_batches))\n",
    "print(len(tokenized_dataset))\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=batch_size,  # Adjust based on your resources\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9uh0lHCBWLe1"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda'\n",
    "class RoPE(nn.Module):\n",
    "  def __init__(self,head_dim=512,max_seq_len=256):\n",
    "    super(RoPE, self).__init__()\n",
    "    self.head_dim = head_dim\n",
    "    freq_com = self.compute_freq(head_dim,max_seq_len)\n",
    "    self.register_buffer('freq_com', freq_com)\n",
    "\n",
    "  def compute_freq(self,head_dim: int,seq_len: int,base:int = 10**4):\n",
    "\n",
    "    exp = -2 * torch.arange(0,head_dim,2).float() / head_dim\n",
    "    thetas = torch.pow(base,exp)\n",
    "    m = torch.arange(0,seq_len).float()\n",
    "    freq = torch.outer(m,thetas).float()\n",
    "    freq_comp = torch.polar(torch.ones_like(freq),freq)\n",
    "\n",
    "    return freq_comp\n",
    "\n",
    "  def apply_rotary_embedding(self,x: torch.Tensor):\n",
    "    x_comp = torch.view_as_complex(x.float().reshape(*(x.shape[:-1]),-1,2))\n",
    "\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    freq_com = self.freq_com[:seq_len,:]\n",
    "    freq_com = freq_com.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotate = x_comp * freq_com\n",
    "    x_out = torch.view_as_real(x_rotate).reshape(*x.shape)\n",
    "    return x_out.float()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.apply_rotary_embedding(x)\n",
    "\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "  def __init__(self,d_model=256,multiple_of=2048):\n",
    "    super(FFN, self).__init__()\n",
    "    hidden = 4*d_model\n",
    "    hidden = int(2*hidden/3)\n",
    "\n",
    "    hidden = multiple_of*((hidden + multiple_of -1)//multiple_of)\n",
    "\n",
    "    self.w1 = nn.Linear(d_model,hidden,bias=False)\n",
    "    self.v = nn.Linear(d_model,hidden,bias=False)\n",
    "    self.w2 = nn.Linear(hidden,d_model,bias=False)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = nn.functional.silu(self.w1(x)) * self.v(x)\n",
    "    return  self.w2(x)\n",
    "\n",
    "import math\n",
    "class MultiHeadGQAttention(nn.Module):\n",
    "  def __init__(self, heads=4,\n",
    "               d_model=256,\n",
    "               seq_len=128,\n",
    "               group_size=2,\n",
    "               max_seq_len=256):\n",
    "    super(MultiHeadGQAttention, self).__init__()\n",
    "    self.heads = heads\n",
    "    self.d_model = d_model\n",
    "    self.group_size = group_size\n",
    "\n",
    "    self.W_q = nn.Linear(d_model,d_model)\n",
    "    self.W_k = nn.Linear(d_model,d_model//group_size)\n",
    "    self.W_v = nn.Linear(d_model,d_model//group_size)\n",
    "    self.W_o = nn.Linear(d_model,d_model)\n",
    "    self.rope = RoPE(head_dim=d_model//heads,max_seq_len=max_seq_len)\n",
    "\n",
    "  def forward(self, q,k,v,mask):\n",
    "    d_k = self.d_model // self.heads\n",
    "    q,k,v = self.W_q(q),self.W_k(k),self.W_v(v)\n",
    "\n",
    "\n",
    "\n",
    "    q = q.view(q.shape[0],q.shape[1],self.heads,-1) \n",
    "\n",
    "    k = k.view(k.shape[0],k.shape[1],self.heads//self.group_size,-1) \n",
    "    v = v.view(v.shape[0],v.shape[1],self.heads//self.group_size,-1) \n",
    "\n",
    "    q = self.rope(q)\n",
    "    k = self.rope(k)\n",
    "\n",
    "    q = q.transpose(1,2)\n",
    "    k = k.transpose(1,2)\n",
    "    v = v.transpose(1,2)\n",
    "\n",
    "\n",
    "    k = k.repeat(1,self.group_size,1,1)\n",
    "    v = v.repeat(1,self.group_size,1,1)\n",
    "\n",
    "    res = torch.matmul(q,k.transpose(-2,-1))  /  math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)\n",
    "      res = torch.masked_fill(res,mask==0,float('-inf'))\n",
    "\n",
    "    attention = nn.functional.softmax(res,dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention,v).transpose(1,2).contiguous().view(q.shape[0],-1,self.d_model)\n",
    "    output = self.W_o(output)\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "               d_model=256,\n",
    "               heads=4,\n",
    "               group_size=2,\n",
    "               max_seq_len=256):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.norm1 = nn.RMSNorm(d_model)\n",
    "    self.norm2 = nn.RMSNorm(d_model)\n",
    "    self.ffn = FFN(d_model=d_model)\n",
    "    self.attention = MultiHeadGQAttention(heads=heads,\n",
    "                                          d_model=d_model,\n",
    "                                          group_size=group_size,\n",
    "                                          max_seq_len=max_seq_len)\n",
    "\n",
    "  def forward(self,x,tgt_causal_mask):\n",
    "    x_norm = self.norm1(x)\n",
    "    x = x + self.attention(x_norm,x_norm,x_norm,tgt_causal_mask)\n",
    "    return x + self.ffn(self.norm2(x))\n",
    "\n",
    "class Llama3(nn.Module):\n",
    "  def __init__(self,vocab_size,\n",
    "               d_model,\n",
    "               heads,\n",
    "               group_size=2,\n",
    "               num_layers=8,\n",
    "               max_seq_len=256,\n",
    "               tokenizer=None,\n",
    "               ignore_index=-100\n",
    "  ):\n",
    "    super(Llama3, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.ignore_index=ignore_index\n",
    "    self.num_layers = num_layers\n",
    "    self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                              heads=heads,\n",
    "                                              group_size=group_size,\n",
    "                                              max_seq_len=max_seq_len) for i in range(num_layers)])\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "    self.norm = nn.RMSNorm(d_model)\n",
    "    self.ffn = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "  @staticmethod\n",
    "  def _build_masks(seq_len,attention_mask):\n",
    "    causal = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool)).to(attention_mask.device)\n",
    "    attention_mask = attention_mask.unsqueeze(1).repeat(1,seq_len,1).int()\n",
    "    return (causal & attention_mask).int()\n",
    "  def __gen_labels(self,labels):\n",
    "    eos_token = torch.full((labels.shape[0], 1), self.tokenizer.eos_token_id, dtype=labels.dtype).to(labels.device)\n",
    "    labels = torch.cat((labels, eos_token), dim=-1)\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    return labels\n",
    "  def calc_loss(self,logits,labels):\n",
    "    loss = nn.functional.cross_entropy(logits.view(-1,logits.shape[-1]),labels.view(-1),ignore_index=self.ignore_index)\n",
    "    return loss\n",
    "\n",
    "  def generate(self,prompt,tokenizer,temp=1.0,top_k=None):\n",
    "    device = 'cuda'\n",
    "    tokenized = tokenizer(prompt, max_length=self.max_seq_len,truncation=True)\n",
    "    tokens = torch.tensor(tokenized['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized['attention_mask']).unsqueeze(0).to(device)\n",
    "    sampled_token = None\n",
    "    i = 0\n",
    "    print(temp)\n",
    "    while sampled_token != tokenizer.eos_token_id and i < 128:\n",
    "      i= i+1\n",
    "      logits = self.__run_model(tokens,attention_mask)[:,-1,:] / temp\n",
    "\n",
    "      probabilities = F.softmax(logits, dim=-1)\n",
    "      new_token = torch.multinomial(probabilities.squeeze(), 1).view(1,1) #torch.argmax(probabilities,dim=-1).unsqueeze(0)\n",
    "      tokens = torch.cat((tokens, new_token), dim=1)\n",
    "      attention_mask = torch.full_like(tokens, fill_value=1, device=device)  # Efficient mask update\n",
    "      sampled_token = new_token.squeeze().item()\n",
    "\n",
    "\n",
    "    tokens = tokens.squeeze().tolist()\n",
    "    tokens = tokens[:-1] if sampled_token == tokenizer.eos_token_id else tokens\n",
    "    return tokenizer.decode(tokens)\n",
    " \n",
    "\n",
    "\n",
    "  def __run_model(self,tgt,attention_mask):\n",
    "    causal_mask = Llama3._build_masks(tgt.shape[1],attention_mask)\n",
    "    tgt_embed = self.embedding(tgt)\n",
    "    for i in range(self.num_layers):\n",
    "      tgt_embed = self.layers[i](tgt_embed,causal_mask)\n",
    "\n",
    "    logits = self.ffn(self.norm(tgt_embed))\n",
    "    return logits\n",
    "\n",
    "\n",
    "  def forward(self,tgt,attention_mask,labels):\n",
    "\n",
    "    labels = self.__gen_labels(labels)\n",
    "   \n",
    "    logits = self.__run_model(tgt,attention_mask)\n",
    "    return self.calc_loss(logits,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "dma_MQKNRefR",
    "outputId": "3e673a64-49da-49fc-a191-708a37550882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "Param. count: 53588306\n",
      "Epoch 1/100, Loss: 6.7794365882873535\n",
      "Epoch 2/100, Loss: 6.0513834953308105\n",
      "Epoch 3/100, Loss: 6.150056838989258\n",
      "Epoch 4/100, Loss: 5.979142189025879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     56\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 207\u001b[0m, in \u001b[0;36mLlama3.forward\u001b[0;34m(self, tgt, attention_mask, labels)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,tgt,attention_mask,labels):\n\u001b[1;32m    205\u001b[0m   labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__gen_labels(labels)\n\u001b[0;32m--> 207\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_loss(logits,labels)\n",
      "Cell \u001b[0;32mIn[14], line 197\u001b[0m, in \u001b[0;36mLlama3.__run_model\u001b[0;34m(self, tgt, attention_mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m tgt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(tgt)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 197\u001b[0m   tgt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(tgt_embed))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 127\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, tgt_causal_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,tgt_causal_mask):\n\u001b[1;32m    126\u001b[0m   x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 127\u001b[0m   x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_causal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 103\u001b[0m, in \u001b[0;36mMultiHeadGQAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     99\u001b[0m   res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(res,mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    101\u001b[0m attention \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(res,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m    104\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(output)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_model=256\n",
    "heads=16\n",
    "num_layers=16\n",
    "group_size=4\n",
    "max_seq_len=256\n",
    "\n",
    "warmup = 100\n",
    "lr = 2.5e-4\n",
    "min_lr = 1e-6\n",
    "\n",
    "class InverseSquareRootLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, init_lr, min_lr=1e-9, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.init_lr = init_lr\n",
    "        self.min_lr = min_lr\n",
    "        super(InverseSquareRootLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step < self.warmup_steps:\n",
    "            lr = self.init_lr * (step / self.warmup_steps)\n",
    "        else:\n",
    "            lr = self.init_lr * math.sqrt(self.warmup_steps / step)\n",
    "\n",
    "        lr = max(lr, self.min_lr)\n",
    "\n",
    "        return [lr for _ in self.base_lrs]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = Llama3(vocab_size=len(tokenizer),\n",
    "               tokenizer=tokenizer,\n",
    "               d_model=d_model,\n",
    "               heads=heads,\n",
    "               group_size=group_size,\n",
    "               num_layers=num_layers,\n",
    "               max_seq_len=max_seq_len).to(device)\n",
    "#model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(),lr=lr,betas=(0.9, 0.98))\n",
    "lr_scheduler = InverseSquareRootLR(optim,warmup,lr,min_lr=min_lr)\n",
    "\n",
    "#train_set = tokenized_dataset['train']\n",
    "print(len(dataloader))\n",
    "print(f\"Param. count: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "for epoch in range(epochs):\n",
    "    for i,batch in enumerate(dataloader):\n",
    "       \n",
    "       # print(batch['attention_mask'])\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        loss = model(inputs,attention_mask,labels)\n",
    "        loss.backward()\n",
    "     #   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optim.step()\n",
    "        lr_scheduler.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sD11ELc3SDR7"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172259/1327297186.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The game began development in painting that as aiqued name named body after\n"
     ]
    }
   ],
   "source": [
    "d_model=256\n",
    "heads=16\n",
    "num_layers=16\n",
    "group_size=4\n",
    "max_seq_len=256\n",
    "\n",
    "warmup = 100\n",
    "lr = 2.5e-4\n",
    "min_lr = 1e-6\n",
    "\n",
    "device = 'cuda'\n",
    "model = Llama3(vocab_size=len(tokenizer),\n",
    "               d_model=d_model,\n",
    "               heads=heads,\n",
    "               group_size=group_size,\n",
    "               num_layers=num_layers,\n",
    "               max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "res = model.generate(\"The game began development in\",tokenizer=tokenizer,temp=1)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
